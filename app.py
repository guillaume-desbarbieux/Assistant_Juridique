import streamlit as st
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import os

st.set_page_config(page_title="Assistant Juridique IA", layout="wide")
st.title("ğŸ“š Assistant Juridique avec IA")
st.write("Posez une question juridique en lien avec le droit du travail, la jurisprudence ou les clauses contractuelles.")

# Champ de saisie utilisateur
user_input = st.text_area("âœ‰ï¸ Votre question :", height=200)

if st.button("ğŸ“¤ Envoyer") and user_input.strip():
    with st.spinner("Recherche et gÃ©nÃ©ration de la rÃ©ponse..."):

        # Charger la base vectorielle
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        db = Chroma(persist_directory="./db", embedding_function=embeddings)
        retriever = db.as_retriever(search_kwargs={"k": 5})

       # RÃ©cupÃ©ration des documents pertinents avec score
        docs_and_scores = retriever.vectorstore.similarity_search_with_score(user_input, k=5)

        # Optionnel : seuil de similaritÃ© Ã  ajuster si besoin
        SIMILARITY_THRESHOLD = 0.7

        # On garde uniquement les documents avec une similaritÃ© suffisante
        filtered_docs = [doc for doc, score in docs_and_scores if score >= SIMILARITY_THRESHOLD]


        # LLM via Ollama
        model_name = "mistral:latest"
        base_url = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")

        # VÃ©rification Ollama accessible
        import requests

        def check_ollama_is_alive():
            try:
                r = requests.get(f"{base_url}/api/generate")
                if r.status_code in [404, 405]:
                    return True
                else:
                    st.error(f"Ollama ne rÃ©pond pas correctement (code {r.status_code})")
                    st.stop()
            except Exception as e:
                st.error(f"Ollama semble injoignable : {e}")
                st.stop()
        check_ollama_is_alive()

        oai = Ollama(model=model_name, base_url=base_url)
        
        # CrÃ©ation du prompt personnalisÃ©
        prompt_template = """
Tu es un assistant juridique expert. Tu dois rÃ©pondre en franÃ§ais, de maniÃ¨re claire et prÃ©cise.
Base ta rÃ©ponse uniquement sur les documents fournis ci-dessous.
Ne fais pas de suppositions en dehors des documents.

Documents:
{context}

Question:
{question}

RÃ©ponse en franÃ§ais :
"""
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=prompt_template
        )

        # CrÃ©ation de la chaÃ®ne LLMChain avec le prompt
        qa_chain = LLMChain(llm=oai, prompt=prompt)

        if not filtered_docs:
            st.warning("â— Aucun document suffisamment pertinent trouvÃ© pour cette question.")
            st.info("L'assistant ne peut pas formuler de rÃ©ponse fiable sans documents de rÃ©fÃ©rence.")
        else:
            try:
                context_text = "\n\n".join([
                    f"[Pertinence : {score:.2f}] {doc.page_content}"
                    for doc, score in docs_and_scores if score >= SIMILARITY_THRESHOLD
                ])

                result = qa_chain.run({"context": context_text, "question": user_input})
                st.subheader("âœ… RÃ©ponse gÃ©nÃ©rÃ©e")
                st.write(result)
            except Exception as e:
                st.error(f"Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse : {e}")
                st.stop()

            st.subheader("ğŸ“ Documents utilisÃ©s")
            for idx, (doc, score) in enumerate(docs_and_scores, 1):
                if score >= SIMILARITY_THRESHOLD:
                    source = os.path.basename(doc.metadata.get('source', 'inconnu'))
                    percent = int(score * 100)
                    st.markdown(f"### ğŸ“„ Document {idx} â€” {source} (ğŸ” Pertinence : {percent}%)")
                    st.code(doc.page_content[:3000], language='markdown')
