import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import os

st.set_page_config(page_title="Assistant Juridique IA", layout="wide")
st.title("üìö Assistant Juridique avec IA")

st.sidebar.markdown("üß† **Mod√®le d'embedding :** `paraphrase-multilingual-mpnet-base-v2`")
st.sidebar.markdown("üóÇÔ∏è **Base vectorielle :** `Chroma`")
st.sidebar.markdown("üí¨ **Mod√®le LLM :** `mistral:latest` via Ollama")


st.sidebar.header("üîß Param√®tres avanc√©s")

max_docs = st.sidebar.slider(
    "Nombre maximal de documents √† utiliser",
    min_value=1,
    max_value=20,
    value=5,
    step=1
)

# Slider de pertinence exprim√© en % (0 = tout passe, 100 = tr√®s strict)
similarity_threshold = st.sidebar.slider(
    "Seuil de pertinence (%)",
    min_value=0,
    max_value=100,
    value=80,
    step=1
)


st.write("Posez une question juridique en lien avec le droit du travail, la jurisprudence ou les clauses contractuelles.")

# Champ de saisie utilisateur
user_input = st.text_area("‚úâÔ∏è Votre question :", height=200)

def distance_to_percent(score, max_dist=10.0):
    """
    Convertit une distance en score de pertinence (%) invers√©e
    en supposant que les distances sont entre 0 et max_dist
    """
    score = max(0, min(score, max_dist))  # clamp
    return round((1 - score / max_dist) * 100)

if st.button("üì§ Envoyer") and user_input.strip():
    with st.spinner("Recherche et g√©n√©ration de la r√©ponse..."):

        # Charger la base vectorielle
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
        db = Chroma(persist_directory="./db", embedding_function=embeddings)
        retriever = db.as_retriever(search_kwargs={"k": max_docs})

       # R√©cup√©ration des documents pertinents avec score
        docs_and_scores = retriever.vectorstore.similarity_search_with_score(user_input, k=max_docs)
        # Ajout de la pertinence (%) √† chaque document
        docs_scores_pertinences = [
            (doc, score, distance_to_percent(score, max_dist=10.0))
            for doc, score in docs_and_scores
        ]

        # Conversion du seuil de pertinence (%) en distance maximale
        max_dist = 10.0
        distance_seuil = max_dist * (1 - similarity_threshold / 100)
        # On garde uniquement les documents avec une distance <= distance_seuil
        filtered_docs = [
            (doc, score, pertinence)
            for doc, score, pertinence in docs_scores_pertinences
            if pertinence >= similarity_threshold
        ]


        # LLM via Ollama
        model_name = "mistral:latest"
        base_url = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")

        # V√©rification Ollama accessible
        import requests

        def check_ollama_is_alive():
            try:
                r = requests.get(f"{base_url}/api/generate")
                if r.status_code in [404, 405]:
                    return True
                else:
                    st.error(f"Ollama ne r√©pond pas correctement (code {r.status_code})")
                    st.stop()
            except Exception as e:
                st.error(f"Ollama semble injoignable : {e}")
                st.stop()
        check_ollama_is_alive()

        oai = Ollama(model=model_name, base_url=base_url)
        
        # Cr√©ation du prompt personnalis√©
        prompt_template = """
Tu es un assistant juridique expert.
Tu dois faciliter le travail des juristes en pr√©sentant les documents qui peuvent leur √™tre utile pour r√©pondre.
Tu dois r√©pondre en fran√ßais, de mani√®re claire et pr√©cise.
Base ta r√©ponse uniquement sur les documents fournis.
Si tu n'as pas assez d'information, dis-le clairement.
Ne fais aucune supposition.

Documents:
{context}

Question:
{question}

R√©ponse en fran√ßais :
"""
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=prompt_template
        )

        # Cr√©ation de la cha√Æne LLMChain avec le prompt
        qa_chain = LLMChain(llm=oai, prompt=prompt)

        if not filtered_docs:
            st.warning("‚ùó Aucun document suffisamment pertinent trouv√© pour cette question.")
            st.info("L'assistant ne peut pas formuler de r√©ponse fiable sans documents de r√©f√©rence.")
        else:
            try:
                context_text = "\n\n".join([
                    f"[Pertinence : {pertinence}%] {doc.page_content}"
                    for doc, score, pertinence in filtered_docs
                ])

                result = qa_chain.run({"context": context_text, "question": user_input})
                st.subheader("‚úÖ R√©ponse g√©n√©r√©e")
                st.write(result)
            except Exception as e:
                st.error(f"Erreur lors de la g√©n√©ration de la r√©ponse : {e}")
                st.stop()

            st.subheader("üìé Documents utilis√©s")
            for idx, (doc, score, pertinence) in enumerate(filtered_docs, 1):
                source = os.path.basename(doc.metadata.get('source', 'inconnu'))
                st.markdown(f"### üìÑ Document {idx} ‚Äî {source} (üîç Pertinence : {pertinence}%)")
                st.markdown(
                    f"""
                    <div style="white-space: pre-wrap; word-wrap: break-word; overflow-x: hidden; background-color: #f9f9f9; padding: 1em; border-radius: 8px; border: 1px solid #ddd;">
                        {doc.page_content}
                    </div>
                    """,
                    unsafe_allow_html=True
                )

        # Affichage debug : tous les documents trouv√©s avec leur score brut
        st.subheader("üõ†Ô∏è Debug : Scores bruts des documents trouv√©s")
        for idx, (doc, score, pertinence) in enumerate(docs_scores_pertinences, 1):
            source = os.path.basename(doc.metadata.get('source', 'inconnu'))
            st.markdown(f"- **Document {idx} ‚Äî {source}** : score brut = {score:.4f}")

